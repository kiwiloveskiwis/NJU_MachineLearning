\documentclass[a4paper,UTF8]{article}
\usepackage{ctex}
\usepackage[margin=1.25in]{geometry}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{hyperref}
\numberwithin{equation}{section}
%\usepackage[thmmarks, amsmath, thref]{ntheorem}
\theoremstyle{definition}
\newtheorem*{solution}{Solution}
\newtheorem*{prove}{Proof}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\usepackage{multirow}

%--

%--
\begin{document}
\title{机器学习导论\\
习题六}
\author{151250104, 卢以宁,  kiwiloveskiwis@gmail.com}
\maketitle

\section{[20pts] Ensemble Methods}
\begin{enumerate}[ {(}1{)}]
\item \textbf{[10pts]} 试说明Boosting的核心思想是什么，Boosting中什么操作使得基分类器具备多样性？
\item \textbf{[10pts]} 试析随机森林为何比决策树Bagging集成的训练速度更快。
\end{enumerate}
\begin{solution}
(1) Boosting 的核心思想是串行对基学习期进行训练，在训练某个基学习期时，根据在此之前的基学习器的表现调整样本分布(即, 预测错误的样例获得更多关注),  从而让当前学习器的训练更有针对性。其中, 改变样本分布的做法使得基学习期具备多样性。

(2) 这是因为随机森林每次只选择一部分属性进行训练。
\end{solution}

\section{[20pts] Bagging}
考虑一个回归学习任务$f:\mathbb{R}^d \rightarrow \mathbb{R}$。假设我们已经学得$M$个学习器$\hat{f}_1(\mathbf{x}),\hat{f}_2(\mathbf{x}),\dots,\hat{f}_M(\mathbf{x})$。我们可以将学习器的预测值看作真实值项加上误差项
\begin{equation}
\hat{f}_m(\mathbf{x})=f(\mathbf{x})+\epsilon_m(\mathbf{x})
\end{equation}
每个学习器的期望平方误差为$\mathbb{E}_{\mathbf{x}}[\epsilon_m(\mathbf{x})^2]$。所有的学习器的期望平方误差的平均值为
\begin{equation}
E_{av}=\frac{1}{M}\sum_{m=1}^M \mathbb{E}_{\mathbf{x}}[\epsilon_m(\mathbf{x})^2]
\end{equation}
M个学习器得到的Bagging模型为
\begin{equation}
\hat{f}_{bag}(\mathbf{x})=\frac{1}{M}\sum_{m=1}^M \hat{f}_m(\mathbf{x})
\end{equation}
Bagging模型的误差为
\begin{equation}
\epsilon_{bag}(\mathbf{x})=\hat{f}_{bag}(\mathbf{x})-f(\mathbf{x})=\frac{1}{M}\sum_{m=1}^M \epsilon_m(\mathbf{x})
\end{equation}
其期望平均误差为
\begin{equation}
E_{bag}=\mathbb{E}_{\mathbf{x}}[\epsilon_{bag}(\mathbf{x})^2]
\end{equation}
\begin{enumerate}[ {(}1{)}]
\item \textbf{[10pts]} 假设$\forall\; m\neq l$，$ \mathbb{E}_{\mathbf{x}}[\epsilon_m(\mathbf{x})]=0$，$ \mathbb{E}_{\mathbf{x}}[\epsilon_m(\mathbf{x})\epsilon_l(\mathbf{x})]=0$。证明
\begin{equation}
E_{bag}=\frac{1}{M} E_{av}
\end{equation}

\item  \textbf{[10pts]} 试证明不需对$\epsilon_m(\mathbf{x})$做任何假设，$E_{bag}\leq E_{av}$始终成立。（提示：使用Jensen's inequality）
\end{enumerate}

\begin{prove} %%%%%%%%%%%%% 2 
(1) 
\begin{equation}
\begin{split}
E_{bag} &=\mathbb{E}_{\mathbf{x}}[\epsilon_{bag}(\mathbf{x})^2]  \\
&= \frac{1}{M^2} \mathbb{E}_{\mathbf{x}}[(\sum_{m=1}^M \epsilon_m(\mathbf{x}))^2]  \\
&= \frac{1}{M^2} \sum_{m=1}^M \mathbb{E}_{\mathbf{x}}[( \epsilon_m(\mathbf{x}))^2]  \\
&=  \frac{1}{M} E_{av}
\end{split}
\end{equation}
\qed
\end{prove}
(2) %%%%%%%%%%%%% 2.2
因为 $f(x) = x^2$ 是凸函数，故由 Jensen's inequality,  $f(\frac{1}{n} (\sum_{i=1}^{n}x_i )) \leq \frac{1}{n} \sum_{i=1}^{n}f(x_i) $

则
\begin{equation}
\begin{split}
E_{bag} &=  \mathbb{E}_{\mathbf{x}}[\frac{1}{M}(\sum_{m=1}^M \epsilon_m(\mathbf{x}))^2]  \\
&\leq \frac{1}{M} \sum_{m=1}^M \mathbb{E}_{\mathbf{x}}[( \epsilon_m(\mathbf{x}))^2]  \\
&=   E_{av}
\end{split}
\end{equation}

\section{[30pts] AdaBoost in Practice}

\begin{enumerate}[ {(}1{)}]
\item \textbf{[25pts]} 请实现以Logistic Regression为基分类器的AdaBoost，观察不同数量的ensemble带来的影响。详细编程题指南请参见链接：
\url{http://lamda.nju.edu.cn/ml2017/PS6/ML6_programming.html}
\item \textbf{[5pts]} 在完成上述实践任务之后，你对AdaBoost算法有什么新的认识吗？请简要谈谈。
\end{enumerate}
\begin{solution}
(1)可能是因为LR并不适合做adaboost的基分类器\\
(2) 线性分类器的线性加权还是线性的，没有提高分类器的VC维（也就是分类性能的量级\\
(3) 决策树桩可能比较合适 \\
(4)那岂不是爆炸了 \footnote{"Boosting shines when there is no terse functional form around": \url{https://stats.stackexchange.com/questions/186966/gradient-boosting-for-linear-regression-why-does-it-not-work}}\\
\end{solution}
\end{document}